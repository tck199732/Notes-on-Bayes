{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "\n",
    "Consider a set of data containing a mixture of $K$ 1d-gaussians with different $\\mu_k$ and $\\sigma_k$. Denote $z_i$ as the particular gaussian that the i-th particle comes from, which is modeled as Dirichlet distribution with some fixed parameter $\\alpha$.  \n",
    "Mathematically, \n",
    "\n",
    "\\begin{gather*}\n",
    "z_i \\sim \\text{Categorical}(\\pi)\\\\\n",
    "\\pi \\sim Dir(\\alpha)\\\\\n",
    "x_i|\\mathbf{\\mu}, z_i \\sim \\mathcal{N}(\\mathbf{c}^T\\mathbf{\\mu}, \\mathbf{\\sigma})\n",
    "\\end{gather*}\n",
    "\n",
    "Let's visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAic0lEQVR4nO3dfXRU9b3v8fdXoOJREB8AI4EZbktreBZipNUiLsWidR2fqyyrtqWHc1s8taftvaK3q/a0dZXbYyutvdrS2kptfWptF6wusQL2VmvxASQiyKFSmdEIVyL1IQUfSPjeP2YnDsmeySSZmb1n5vNaa6/M/GbvmV92SL78fvu3v19zd0REROLmkKg7ICIiEkYBSkREYkkBSkREYkkBSkREYkkBSkREYkkBSkREYmlwbzuY2VDgEeDQYP/fuPsNZnY0cC+QBFLAJ9z9teCY64AFQAfwBXf/Q9A+E7gDOAx4ALjGe1nnfuyxx3oymezHtyYiInG3YcOGV919ZNhr1tt9UGZmwOHu/g8zGwL8GbgGuBD4u7svMbPFwFHufq2ZTQTuBpqA44E1wAfdvcPMngyOfZxMgPqBu6/K9/mNjY2+fv36vny/IiJSIcxsg7s3hr3W6xSfZ/wjeDok2Bw4D1getC8Hzg8enwfc4+7vuPsOYDvQZGZ1wHB3XxeMmn6RdYyIiMhBCroGZWaDzKwZ2A2sdvcngNHuvgsg+Doq2H0M8FLW4S1B25jgcfd2ERGRHgoKUO7e4e7TgXoyo6HJeXa3sLfI097zDcwWmtl6M1vf2tpaSBdFRKTK9LpIIpu7v25m/xeYB7xiZnXuviuYvtsd7NYCjM06rB7YGbTXh7SHfc4yYBlkrkH1pY8iUrv2799PS0sLb7/9dtRdkW6GDh1KfX09Q4YMKfiYQlbxjQT2B8HpMOBM4H8DK4GrgCXB1xXBISuBu8zse2QWSUwAngwWSbSZ2SzgCeBK4JaCeyoi0ouWlhaGDRtGMpkks75L4sDd2bNnDy0tLYwfP77g4woZQdUBy81sEJkpwfvc/fdmtg64z8wWAC8ClwQd2WJm9wHPAe3AInfvCN7rc7y3zHxVsImIFMXbb7+t4BRDZsYxxxxDXy/Z9Bqg3H0TcGJI+x7gjBzH3AjcGNK+Hsh3/UpEZEAUnOKpPz8XZZIQEZFYUoASEZFYUoASGYDO6x1hm1J0yUB95jOfYdSoUUyenP/KSDKZZMqUKUyfPp3GxveSMtx8881MmjSJyZMnM3/+/INWN3Y/Ztu2bUyfPr1rGz58OEuXLu3a//XXX+fiiy/mhBNOoKGhgXXr1vHSSy9x+umn09DQwKRJk/j+979f3BPg7rHeZs6c6SJRSiQSTuaevR5bIpHIeRxdiVikXJ577rmDGxIJdyjelufnXQp/+tOffMOGDT5p0qS8+yUSCW9tbT2oraWlxZPJpO/bt8/d3S+55BL/+c9/nveYTu3t7T569GhPpVJdbVdeeaX/5Cc/cXf3d955x1977TXfuXOnb9iwwd3d33zzTZ8wYYJv2bIlZz97/HzcHVjvOf7+awQl0ot0Op3zP1CpVCrq7kk+6XQxw1Pm/QowZ84ctm3bBsCePXt6HQHlMnv2bI4++uh+HQvQ3t7OW2+9RXt7O/v27eP4448v6Li1a9fy/ve/n0QiAcCbb77JI488woIFCwB43/vex4gRI6irq2PGjBkADBs2jIaGBl5++eV+97c7BSgRkSLbvn07EyZMAGDTpk1MmTLloNc/+tGPHjSd1rmtWbOmX59nZpx11lnMnDmTZcuWATBmzBi+8pWvMG7cOOrq6jjyyCM566yz8h7T6Z577mH+/Pldz1944QVGjhzJpz/9aU488UQ++9nPsnfv3oOOSaVSbNy4kZNPPrlf30OYPmWSEJHCJRKJ0KW1iURCI68qlk6nGTNmDIcckvn//6ZNm5g6depB+zz66KNF/czHHnuM448/nt27dzN37lxOOOEEpkyZwooVK9ixYwcjRozgkksu4Ze//CWf/OQncx4ze/Zs3n33XVauXMm3v/3trvdvb2/n6aef5pZbbuHkk0/mmmuuYcmSJXzzm98E4B//+AcXXXQRS5cuZfjw4UX7vjSCEimRVCoVOi2YLnCaSCpTc3PzQQFpw4YNPQJUsUdQnVN3o0aN4oILLuDJJ59kzZo1jB8/npEjRzJkyBAuvPBC/vKXv+Q9BmDVqlXMmDGD0aNHd+1bX19PfX191+jo4osv5umnnwYy6aUuuugiLr/8ci688MJ+9T8XBSiRMuscWWnlX3V65plnulbLPf/886xYsaLHFN+jjz5Kc3Nzj+3MM8/s8+ft3buXtra2rscPPfQQkydPZty4cTz++OPs27cPd2ft2rU0NDTkPQbg7rvvPmh6D+C4445j7NixXdfV1q5dy8SJE3F3FixYQENDA1/60pf63PfeKECJlFmukZVGV9WhubmZAwcOMG3aNL7xjW/Q0NDA8uXLez8wxPz58/nwhz/Mtm3bqK+v5/bbb+967ZxzzmHnzp288sornHrqqUybNo2mpiY+/vGPM2/ePE4++WQuvvhiZsyYwZQpUzhw4AALFy4EyHnMvn37WL16dehI6JZbbuHyyy9n6tSpNDc3c/311/PYY49x55138vDDD3eNAh944IH+nbgQvVbUjZoq6krUzIxy/Z6U87Oq0datW7tGCQAkkwWvvCtIIgG9XD/8wAc+wMaNGxk2bFjxPrdK9Pj5kL+irhZJiEj1KvNilLa2Ng455BAFpyLRFJ+ISJEMGzaMv/71r1F3o2ooQInEiBZQiLxHU3wiMZLv/iiVkZBaoxGUiIjEkgKUCPmzknfmIxOR8tIUnwjvJYQVkfjQCEpERGJJAUpERGJJAUpERGJJAUpEJIbefvttmpqamDZtGpMmTeKGG27IuW9fS77nKtWeqz1fOfiSln3PlbQyLptKvks5UAHl2Suhj1HrXlK8kiu+HzhwwNva2tzd/d133/WmpiZft25d6L59Lfmeq1R7ISXcu5eD70vZd5V8FxEJRFTxvSgl382MI444AsjUXNq/f3+fb9bOVfI9V6n2Qkq4dy8HX8qy7wpQIiJFVqyS7x0dHUyfPp1Ro0Yxd+7cnOXU+1PyvVOuUu252ruXgy/kmP7SfVAiIkVUzJLvgwYNorm5mddff50LLriAzZs3h47G+lPyHXKXas/VHlYOvrdjBkIjKBGRIipFyfcRI0YwZ84cHnzwwdDX+1PyPVep9nwl3MPKwfd2zEBoBCUiUkRhJd+/9a1vHbRPISOo1tZWhgwZwogRI3jrrbdYs2YN1157bY/99u7dy4EDBxg2bFhX+favfe1rHHXUUV0l3w877DDWrl3btcLPc5Rqz9XeKawcfG/HDIRGUCIiRVSsku+7du3i9NNPZ+rUqZx00knMnTuXc889t+v1gZR8z1WqPV8J91zl4EtZ9l0l30WojFLrldDHqHUvKR5BxXeVfM+jryXfex1BmdlYM/ujmW01sy1mdk3Q/nUze9nMmoPtnKxjrjOz7Wa2zcw+ltU+08yeDV77ganAjUjBVMyw71Kp4i4z7y04qeR7cRVyDaod+LK7P21mw4ANZrY6eO1md78pe2czmwhcBkwCjgfWmNkH3b0DuA1YCDwOPADMA1YV51sRyS+ZTJLO8d/pSiipoWKG8aeS78XVa4By913AruBxm5ltBcbkOeQ84B53fwfYYWbbgSYzSwHD3X0dgJn9AjgfBSgpE5XUEKksfVokYWZJ4ETgiaDpajPbZGY/M7OjgrYxwEtZh7UEbWOCx93bwz5noZmtN7P1ra2tfemiiIhUiYIDlJkdAdwPfNHd3yQzXfd+YDqZEdZ3O3cNOdzztPdsdF/m7o3u3jhy5MhCuygiIlWkoABlZkPIBKdfuftvAdz9FXfvcPcDwE+ApmD3FmBs1uH1wM6gvT6kXUREpIdCVvEZcDuw1d2/l9Vel7XbBcDm4PFK4DIzO9TMxgMTgCeDa1ltZjYreM8rgRVF+j5ERKTKFLKK7xTgCuBZM2sO2q4H5pvZdDLTdCngXwHcfYuZ3Qc8R2YF4KJgBR/A54A7gMPILI7QAgkREQlVyCq+PxN+/SjnrcLufiNwY0j7eqDveedFRKTmKNWRSBXQTbxSjRSgRKpAKpXKWZU6183JEm+FlFLPt09fSsbHlbKZi0jVSi5Nkn6jeAE6cWSC1BdTRXu/fAYPHsx3v/tdZsyYQVtbGzNnzmTu3LlMnDixoH0OPfRQHn74YY444gj279/Pqaeeytlnn82sWbPK0v9iUIASkaqVfiON31C87CH2H4WllJozZw4//vGP+dCHPsSePXs47bTT2Lx5c+8HZqmrq6OuLrNYOruUenaAyrdPMUrGR00BSqpKpefbk+pQSMn3tra2HsfddNNNnHnmmT3aCymlHrZPR0cHM2fOZPv27SxatKhopdjLRQFKqory7UnUilnyHQorpZ5rn0JLxseVApSISBGFlXy/9NJLD9qn0BFUIaXUC9knu2S8ApSISI0qVsn3Qkqp59un0JLxcaZl5iIiRVSsku+9lVI/55xzePTRR3Pu01vJ+EqgEZSIVK3EkYmCV94V+n692bRpU1FKvp966ql5r6d2BqJc+0ydOpWNGzcOqA9RU4ASqXKdWSZyvZavUm+lK9c9S51U8r24FKBEqpxKxZePSr4Xl65BiYhILClAiYhILClAiYhILClAiYhILClAiYhILClAiYhILClAiYgUyeuvv86tt96ad59UKsVdd93V63ulUqmKyptXCgpQIlK1kskkZla0LZlM5v28YgYo0Y26IlLFil1+pbcbmxcvXszf/vY3pk+fzty5cwFYtWoVZsZXv/pVLr30UhYvXszWrVuZPn06V111FRdccAFXXHEFe/fuBeCHP/whH/nIR4rW50qmACUiUiRLlixh8+bNNDc3c//99/OjH/2IZ555hldffZWTTjqJ2bNns2TJEm666SZ+//vfA7Bv3z5Wr17N0KFDef7555k/fz7r16+P+DuJBwUoEZES+POf/8z8+fMZNGgQo0eP5rTTTuOpp57qUXRw//79XH311TQ3NzNo0CClSsqiACUiUgKFTi3efPPNjB49mmeeeYYDBw4wdOjQEvescmiRhIhIkQwbNqyrUu7s2bO599576ejooLW1lUceeYSmpqaD9gF44403qKur45BDDuHOO++ko6Mjqu7HjkZQIiJFcswxx3DKKacwefJkzj77bKZOncq0adMwM77zne9w3HHHccwxxzB48GCmTZvGpz71KT7/+c9z0UUX8etf/5rTTz+dww8/POpvIzasmCtcSqGxsdF1wVAKZWZFXbVV7artfG3dupWGhoau58lkknQ6XbT3r/b6WaXW/ecDYGYb3L0xbH+NoESkaimYVDZdgxIRkVjqNUCZ2Vgz+6OZbTWzLWZ2TdB+tJmtNrPng69HZR1znZltN7NtZvaxrPaZZvZs8NoPTOU8RUQkh0JGUO3Al929AZgFLDKzicBiYK27TwDWBs8JXrsMmATMA241s0HBe90GLAQmBNu8In4vIiJVdU2tmvTn59JrgHL3Xe7+dPC4DdgKjAHOA5YHuy0Hzg8enwfc4+7vuPsOYDvQZGZ1wHB3X+eZnv4i6xgRiUAikehXzrm4Gjp0KHv27FGQihl3Z8+ePX2+x6tPiyTMLAmcCDwBjHb3XcGH7zKzUcFuY4DHsw5rCdr2B4+7t4tIRHItIqjU2ff6+npaWlpobW2NuivSzdChQ6mvr+/TMQUHKDM7Argf+KK7v5nnH3DYC56nPeyzFpKZCmTcuHGFdlFEatyQIUMYP3581N2QIiloFZ+ZDSETnH7l7r8Nml8Jpu0Ivu4O2luAsVmH1wM7g/b6kPYe3H2Zuze6e+PIkSML/V5ERKSKFLKKz4Dbga3u/r2sl1YCVwWPrwJWZLVfZmaHmtl4MoshngymA9vMbFbwnldmHSMiInKQQqb4TgGuAJ41s+ag7XpgCXCfmS0AXgQuAXD3LWZ2H/AcmRWAi9y9M7nU54A7gMOAVcEmIiLSg1IdSVWpttQ9UdF5lHLJl+pImSRERCSWFKCk4iSTydB7d8yMRCIRdfeqQq77oyr5HimpPEoWKxUnnU5r+qnE8iVZrdR7pKTyaAQlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlIiKxpAAlsZUra7kylovUBmUzl9hS1nKR2qYRlIiIxJIClIiIxJIClIiIxJIClIiIxJIClIj0SSKRCF1daWYkk8mouydVRKv4RKRPUqlUztfMrHwdkaqnEZSIiMSSApSIiMSSApSIiMSSApSIiMSSApSIiMSSApSIiMSSApSIiMRSrwHKzH5mZrvNbHNW29fN7GUzaw62c7Jeu87MtpvZNjP7WFb7TDN7NnjtB6YbJkREJI9CRlB3APNC2m929+nB9gCAmU0ELgMmBcfcamaDgv1vAxYCE4It7D1FRESAAgKUuz8C/L3A9zsPuMfd33H3HcB2oMnM6oDh7r7OMwV+fgGc388+i4hIDRjINairzWxTMAV4VNA2Bngpa5+WoG1M8Lh7eygzW2hm681sfWtr6wC6KCIilaq/Aeo24P3AdGAX8N2gPey6kudpD+Xuy9y90d0bR44c2c8uiohIJetXgHL3V9y9w90PAD8BmoKXWoCxWbvWAzuD9vqQdhERkVD9ClDBNaVOFwCdK/xWApeZ2aFmNp7MYogn3X0X0GZms4LVe1cCKwbQbxERqXK9ltsws7uBOcCxZtYC3ADMMbPpZKbpUsC/Arj7FjO7D3gOaAcWuXtH8FafI7Mi8DBgVbCJiIiE6jVAufv8kObb8+x/I3BjSPt6YHKfeidVL5lMkk6nQ19LJBJl7o0MVGcxw1yv5aslJdKdChZKpNLpNJk7D6QaqJihFJNSHYmISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQImISCwpQEnJJZNJzCx0SyQSUXev5iWTYNZzSyaj7pnUOgUoKbl0Oo27h26pVCrq7tW8dBrce26goCXRUoCS2pBrmKChQ06pVM+glU73//0SiUTOkXRS51xCDI66AyJl0TlMKIRZafsSkWQyPMCUa5Y132jZqvScy8AoQIl0l0iEB6lEIjOsqFB9idEicaAAJdUnbKjQl2FCriCUFbSSS5Ok3+j/fFfiyASpL+b4HBEBCghQZvYz4Fxgt7tPDtqOBu4FkkAK+IS7vxa8dh2wAOgAvuDufwjaZwJ3AIcBDwDXuOv/c1ICZRgqpN9I4zf0/zPsP6prSivf9GEFDzolYoUskrgDmNetbTGw1t0nAGuD55jZROAyYFJwzK1mNig45jZgITAh2Lq/p4jEXOfsZ/cNwlcCDmRRhUivAcrdHwH+3q35PGB58Hg5cH5W+z3u/o677wC2A01mVgcMd/d1wajpF1nHiEiFCFvZ565RkpRGf69BjXb3XQDuvsvMRgXtY4DHs/ZrCdr2B4+7t4vE2kHXmr4OBFNziSMHtvQtcWQidJpP16ZE3lPsRRJhE+uepz38TcwWkpkOZNy4ccXpmUg/HHStyaxo17ZyBaFiXZsa6DoRkTjo7426rwTTdgRfdwftLcDYrP3qgZ1Be31Ieyh3X+buje7eOHLkyH52UaR2hWWH0DScVJr+BqiVwFXB46uAFVntl5nZoWY2nsxiiCeD6cA2M5tlmTvyrsw6RqQy5FohoCwIIiVRyDLzu4E5wLFm1gLcACwB7jOzBcCLwCUA7r7FzO4DngPagUXu3hG81ed4b5n5qmAT6b9yp0Yo4P6ocos6O4RIKfUaoNx9fo6Xzsix/43AjSHt64HJfeqdSD5Fvt8p7ObbgS6G6KuwxRP5Fk5US3aIzjx9uV5TUuHapEwSEn9lGiYM9ObbYggLRNV2U28Y5emTMApQEn/VMkwQkT5RuQ0pChUlFJFi0whKiqKzKKGISLFoBCUyUGHLz7X0XGTANIISGaiwC/y6sC8yYBpBiYhILGkEJfFShiRyuYoNlvuep1oQVpxYNaKkUApQEi8VUGwwCpWaMUKznzIQClAiFUC3gkktUoASibnEkQnSXzfsP3q2q3aUVDMFKKlqcciv1xfhU3mp0Os2yaXJPuXtE6k0ClASjWrPrxe2OqCzPc8Kgb5M5VVq3r5+nhqpQQpQEo1qv6gSw9IccaFTI4XSfVAiIhJLClAiEmudtaLCtqRSSlU1TfGJSKypVlTt0ghKpJyUWFakYBpBiZSTUiuIFEwBSqqC8uuJVB8FKKkKlZhfTw6m+6OkOwUoEYkF3R8l3SlASWlVahruMnjv1DgEf4QHeloSRyZCs0koBZJUIgUoKa1qzxgxAF2nxqxo5yhXEKqEFEgi3WmZuYiIxJIClBQsmUzmvKM/oSm7/gu7N0r3RxUkV5YJZZioDprik4Kl02lc03XFV4bVAWHXpqrhulSuLBPKMFEdFKBEakClluYALT+vZQpQIhJrWn5eu3QNSkREYmlAIygzSwFtQAfQ7u6NZnY0cC+QBFLAJ9z9tWD/64AFwf5fcPc/DOTzpTZVWhl3EemfYkzxne7ur2Y9XwysdfclZrY4eH6tmU0ELgMmAccDa8zsg+7eUYQ+SA2ptLRGuldZpH9KMcV3HrA8eLwcOD+r/R53f8fddwDbgaYSfL5EJZnsuVRaf4W7bsjtvvV6gV/Lz6XGDTRAOfCQmW0ws4VB22h33wUQfB0VtI8BXso6tiVo68HMFprZejNb39raOsAuStmE/SXWMqv+S6XCI1vYcKwGqbRW9RvoFN8p7r7TzEYBq83sv/LsG7bmJnSext2XAcsAGhsbK2cuR0TKRqW1qt+ARlDuvjP4uhv4HZkpu1fMrA4g+Lo72L0FGJt1eD2wcyCfLyIi1avfIygzOxw4xN3bgsdnAd8AVgJXAUuCryuCQ1YCd5nZ98gskpgAPDmAvksJJJNJ0jmmkMqdzkhFCEVq20Cm+EYDvwtSigwG7nL3B83sKeA+M1sAvAhcAuDuW8zsPuA5oB1YpBV88ROndEaVtlpPRIqr3wHK3V8ApoW07wHOyHHMjcCN/f1MERGpHcokISJVJ1eWc2U6ryzKxSdSJJV2Q241V9/NleUclOm8kihAiRRJpRUPVvVdiTsFKOm7ShsqSJ9Ua+0oqTwKUNJ3JRgqKAFsH4QVSCpicaRKrh0l1UUBSmJBS8r7QCkUclJxw+qiACXSD2GznJrhjJ6KG1YXBagaFKdsEZUqdgsiSjx00HUpiYICVA3qU7YIDRUqQ4mHDrouJVFQgJL8YjdUEOm77gPMzse6NhVvClBSVpWWAFYr6qtDdhAye+//XLo2FW8KUFJWlbZaTwPI6tOZBqlTdpBKJBJ5s1BIeSlAiUhNyQ5A2aOpzHMNqeJEyWIlI5nsWT/bTHNZIhIZjaAkQ3NZ1Ul3rkoFU4CqYrnud9K9TjWkhMvPqzkbusSDAlQVi7o6biXl19Nqvb5TNnQpNQWoWlPGv8SVtGJPM5y1qecMaCLnQgmt8Cs/BahaU6K/xJU0WhJKnhG9UvT8dlM9VvZ10gq/8lOAkqKopNGSoIzoUhEUoESkqCo9sWyuhY9SfgpQUnOU/7a0Kj2xrEp2xIdu1K1wyWQSMwvdEoMG6cbbEJ2X4bK3Grv0Iv2SyPm7lkwmo+5cVdIIqsLlXUqe62rvAFRaslfpA93U24tUzl8nLaAoDQWoChCnAoOVtBhC9zb1kW7qzStf/JbSUICqAL3ecFuiv8SVvnRc9zYVSRGWpFfDTb26NlV+ClCVJF8gKsFf4koaLUkJlXBJeqWv+OvUvYRH99d0g2//KEBVEg0JJC6KdL2q0lf8QeZbTqdToe2plK5PDYQClFSMXAPIXHRtoIQ039Wlt1Oh0VX/lT1Amdk84PvAIOCn7r6k3H2Io7wLIaBoS8RzrcIL/dyYXW/SALI2VMOCCsgeZKZy7pNOW9evtuJUT2UNUGY2CPg/wFygBXjKzFa6+3Pl7EesBMOCNHDQ394B/ovNtxy8Eq4r6WbaClWEqb9cQSi5NFnw9F8cglkh324ymSCdNtJplZ4PU+4RVBOw3d1fADCze4DzgKoJUPlGQvkkBhCQcq22q9RABCVb9yGlluvfcGfF5kLk+F3oS8AJC2ZxCFrddQah7r8H6XRS04KUP0CNAV7Ket4CnFzqD+3tPqJcP+z+BJsE3UZCB39Yj1+8zuCSJt3vi8OlDkb5gkihvycKRDWuL39QSxTM4jwCC8uqDuG/N/mCV36JrvetlClFK2dBOzO7BPiYu382eH4F0OTu/9Ztv4XAwuDph4BtZeriscCrZfqsSqFzEk7npSedk3A6Lz1ln5OEu48M26ncI6gWYGzW83pgZ/ed3H0ZsKxcnepkZuvdvbHcnxtnOifhdF560jkJp/PSU6HnpNzJYp8CJpjZeDN7H3AZsLLMfRARkQpQ1hGUu7eb2dXAH8gsM/+Zu28pZx9ERKQylP0+KHd/AHig3J9boLJPK1YAnZNwOi896ZyE03npqaBzUtZFEiIiIoVSwUIREYklBagsZvafZvZfZrbJzH5nZiOi7lOUzGyemW0zs+1mtjjq/kTNzMaa2R/NbKuZbTGza6LuU1yY2SAz22hmv4+6L3FhZiPM7DfB35StZvbhqPsUB2b278Hvz2Yzu9vMhubaVwHqYKuBye4+FfgrcF3E/YlMVlqqs4GJwHwzmxhtryLXDnzZ3RuAWcAinZMu1wBbo+5EzHwfeNDdTwCmofODmY0BvgA0uvtkMovlLsu1vwJUFnd/yN3bg6ePk7lPq1Z1paVy93eBzrRUNcvdd7n708HjNjJ/cMZE26vomVk98HHgp1H3JS7MbDgwG7gdwN3fdffXI+1UfAwGDjOzwcA/EXIvbCcFqNw+A6yKuhMRCktLVfN/jDuZWRI4EXgi4q7EwVLgfwIHIu5HnPw3oBX4eTD1+VMzOzzqTkXN3V8GbgJeBHYBb7j7Q7n2r7kAZWZrgrnP7tt5Wfv8LzLTOb+KrqeRC0v2pSWfgJkdAdwPfNHd34y6P1Eys3OB3e6+Ieq+xMxgYAZwm7ufCOwFdB3X7CgyMzHjgeOBw83sk7n2r7mChe5+Zr7Xzewq4FzgDK/tNfgFpaWqNWY2hExw+pW7/zbq/sTAKcA/m9k5wFBguJn90t1z/tGpES1Ai7t3jrB/gwIUwJnADndvBTCz3wIfAX4ZtnPNjaDyCYopXgv8s7vvi7o/EVNaqm4sk0L6dmCru38v6v7Egbtf5+717p4k82/kYQUncPf/B7xkZh8Kms6gisoKDcCLwCwz+6fg9+kM8iweqbkRVC9+CBwKrA7S2T/u7v892i5FQ2mpQp0CXAE8a2bNQdv1QXYUke7+DfhV8B+8F4BPR9yfyLn7E2b2G+BpMpdRNpInq4QySYiISCxpik9ERGJJAUpERGJJAUpERGJJAUpERGJJAUpERGJJAUokYkGW9B1mdnTw/KjgeSLqvolESQFKJGLu/hJwG7AkaFoCLHP3dHS9Eome7oMSiYEghdIG4GfAvwAnBlnkRWqWMkmIxIC77zez/wE8CJyl4CSiKT6RODmbTAmCyVF3RCQOFKBEYsDMpgNzyVTq/Xczq4u2RyLRU4ASiViQ1fk2MvWlXgT+k0xRN5GapgAlEr1/AV5099XB81uBE8zstAj7JBI5reITEZFY0ghKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERi6f8DDPBuN5dgVu8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y, centers = datasets.make_blobs(n_samples=50000, n_features=1, centers=3, center_box=(-5,7), return_centers=True, random_state=0)\n",
    "bins = 50\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X[np.where(y==0)], bins=bins, color='r', histtype='step', label=f'$\\mu=${centers[0][0]:2f}')\n",
    "ax.hist(X[np.where(y==1)], bins=bins, color='b', histtype='step', label=f'$\\mu=${centers[1][0]:2f}')\n",
    "ax.hist(X[np.where(y==2)], bins=bins, color='g', histtype='step', label=f'$\\mu=${centers[2][0]:.2f}')\n",
    "ax.hist(X, bins=bins, color='k', label='total', histtype='step')\n",
    "ax.set(xlabel='X')\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the figure, the data we can observe would be the sum of the gaussian distributions. If these gaussians are close then we could not distinguish them from being one gaussian or a mixture of gaussian. Our goal, as usual, is to calculate the posterior given the data. The problem would be as easy as one single gaussian if we know which gaussian each data point comes from. In reality we don't, thus we need to introduce the latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean field Variational inference for GMM\n",
    " In our case, we know or we want to group the data into $K$ gaussian, what we want to learn is the posterior for variable $X$, latent variables $Z$ which tells us which gaussian it belongs to, and finally $\\pi$ the pdf of these gaussians. Our probablistic model is\n",
    "\\begin{equation*}\n",
    "p(X, Z, \\pi|\\mu, \\sigma) = p(\\pi)\\prod_{i=1}^{N} p(z_i|\\pi) p(x_i|z_i,\\mu,\\sigma) = Dir(\\pi|\\alpha) \\prod_{i=1}^N \\prod_{k=1}^K \\big[\\pi_k \\mathcal{N}(x_i|\\mu_k, \\sigma_k)\\big]^{z_{ik}}\n",
    "\\end{equation*}\n",
    "Note that $z_{ik}$ is either 1 or 0 to pick the gaussian from which the data point belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for conjugacy\n",
    "First, we need to verify that GMM satisfy conditional conjugacy. Namely, if we fix $Z$, the prior has the form\n",
    "\\begin{gather*}\n",
    "\\text{prior:} \\qquad p(\\pi,Z) = p(\\pi) \\prod_{i=1}^N p(z_i|\\pi) = Dir(\\pi|\\alpha) \\prod_{i=1}^N\\prod_{k=1}^K \\pi_k^{z_{ik}} = C\\prod_{k=1}^K \\pi_k^C \\\\\n",
    "\\text{posterior:} \\qquad p(Z,\\pi|X) \\propto p(X,Z,\\pi) = Dir(\\pi|\\alpha)  \\prod_{i=1}^N\\prod_{k=1}^K C^{z_{ik}} \\pi_k^{z_{ik}} = C\\prod_{k=1}^K \\pi_k^C\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\prod_{i=1}^N \\pi_k^{z_{ik}} \\rightarrow \\prod_{i=1}^N \\pi_k^C \\rightarrow \\pi_k^C$, all C represent differenet constants. If we fix $\\pi$, then\n",
    "\\begin{gather*}\n",
    "\\text{prior:} \\qquad p(\\pi,Z) = p(\\pi) \\prod_{i=1}^N p(z_i|\\pi) = Dir(\\pi|\\alpha) \\prod_{i=1}^N\\prod_{k=1}^K \\pi_k^{z_{ik}} = C\\prod_{i=1}^N\\prod_{k=1}^K C^{z_{ik}} \\\\\n",
    "\\text{posterior:} \\qquad p(Z,\\pi|X) \\propto p(X,Z,\\pi) = Dir(\\pi|\\alpha)  \\prod_{i=1}^N\\prod_{k=1}^K C^{z_{ik}} \\pi_k^{z_{ik}} = C \\prod_{i=1}^N\\prod_{k=1}^K C^{z_{ik}}\n",
    "\\end{gather*}\n",
    "Therefore, it is ok to apply block coordinate ascent. Next we proceed to get the update rule for $q(Z)$ and $q(\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Algorithm\n",
    "In a mean-field approximation, we neglect any correlation between the latent variables, very much like the mean-field approximation in many body problem which allows us to diagonalize the Hamiltonian. Mathematically, $q(Z,\\pi) = q(Z)q(\\pi)$. Here, the model involves X, latent variables Z and $\\pi$ and parameters of the gaussians. In case like this, it is suggested to use the Expectation-Maximization (EM) algorithm in which the E-step updates the pdf of latent variables (using Coordinate Ascent for Variational Inference) and the M-step optimizes the parameters. In this ipynb, I take this approach as axiom and possibly will study the 'proof' later. \n",
    "\n",
    "#### E-step : Update Rule for $q(Z)$ and $q(\\pi)$\n",
    "The alogorithm of \"Coordinate Ascent for Variational Inference\" tells us the update rule for each latent variables : \n",
    "\\begin{equation*}\n",
    "q_k(\\theta_k) \\propto \\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta,x) \\bigg]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, \n",
    "\\begin{align*}\n",
    "\\log q(\\pi) &= \\mathbb{E}_{q(Z)}\\log p(X, Z, \\pi) + \\text{const} \\\\ \n",
    "&= \\mathbb{E}_{q(Z)} \\bigg[ \\sum_{k=1}^K (\\alpha_k-1)\\log \\pi_k + \\sum_{i=1}^N\\sum_{k=1}^K z_{ik}\\log \\pi_k \\bigg] + \\text{const}\\\\\n",
    "&= \\sum_{k=1}^K (\\alpha_k-1)\\log \\pi_k + \\sum_{i=1}^N\\sum_{k=1}^K \\mathbb{E}_{q(Z)}z_{ik} \\log \\pi_k + \\text{const} \\\\\n",
    "&= \\sum_{k=1}^K \\log \\pi_k \\bigg( \\alpha_k -1 + \\sum_{i=1}^N \\mathbb{E}_{q(Z)}z_{ik} \\bigg) + \\text{const}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have used the relation $Dir(\\pi|\\alpha) \\propto \\prod_{k=1}^K \\pi_k^{\\alpha_k-1}$ before taking the log. Due to the conditional conjugacy $q(\\pi)$ would have the same form as prior and thus we get the update rule :\n",
    "\\begin{gather*}\n",
    "q(\\pi) = Dir(\\pi | \\alpha')\\\\\n",
    "\\alpha_k' = \\alpha_k + \\sum_{i=1}^N \\mathbb{E}_{q(Z)} z_{ik}  = \\alpha_k + \\sum_{i=1}^N q(z_{ik}=1)\n",
    "\\end{gather*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for $q(Z)$:\n",
    "\\begin{align*}\n",
    "    \\log q(Z) &= \\mathbb{E}_{q(\\pi)}\\log p(X, Z, \\pi) + \\text{const} \\\\ \n",
    "    &= \\mathbb{E}_{q(\\pi)} \\sum_{i=1}^N\\sum_{k=1}^K z_{ik}\\bigg[\\log \\pi_k + \\log \\mathcal{N}(x_i| \\mu_k,\\sigma_k) \\bigg] + \\text{const}\\\\\n",
    "    &= \\sum_{i=1}^N\\sum_{k=1}^K z_{ik}\\bigg[\\mathbb{E}_{q(\\pi)} \\log \\pi_k + \\log \\mathcal{N}(x_i| \\mu_k,\\sigma_k) \\bigg] + \\text{const} \\\\\n",
    "    &= \\sum_{i=1}^N\\sum_{k=1}^K z_{ik}\\log \\rho_{ik} + \\text{const}\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we denote $\\log \\rho_{ik} = \\mathbb{E}_{q(\\pi)} \\log \\pi_k + \\log \\mathcal{N}(x_i| \\mu_k,\\sigma_l)$ since again by conjugacy we know $q(Z)$ is categorical. We can see that \n",
    "\\begin{gather*}\n",
    "q(Z)\\propto \\prod_{i=1}^N\\prod_{k=1}^K \\rho_{ik}^{z_{ik}}\n",
    "\\end{gather*}\n",
    "\n",
    "We can then normalize by using $\\sum_{k=1}^K p(z_{ik}=1) = \\sum_{k=1}^K \\rho_{ik}= 1$, i.e.\n",
    "\\begin{gather*}\n",
    "q(Z)= \\prod_{i=1}^N\\prod_{k=1}^K \\bigg(\\frac{\\rho_{ik}}{\\sum_{l=1}^K \\rho_{il}}\\bigg)^{z_{ik}} \\\\\n",
    "\\log \\rho_{ik} = \\mathbb{E}_{q(\\pi)} \\log \\pi_k + \\log \\mathcal{N}(x_i|\\mu_k,\\sigma_k)\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    "- the expectation term in $\\rho_{ik}$ can be calculated with the update rule of $q(\\pi)$. \n",
    "- the expectaton of Dirichlet distribution is given by di-gamma function $\\psi(\\alpha_k) - \\psi(\\sum_i \\alpha_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M-step : Maximize ELBO with respect to parameters\n",
    "In M-step, we use the $q(Z)$ found in E-step to optimize:\n",
    "\\begin{equation*}\n",
    "\\theta^{\\text{NEW}} = \\argmax_{\\theta} \\mathbb{E}_{p(Z)} \\log p(X,Z; \\theta)\n",
    "\\end{equation*}\n",
    "The expectation value, dropping the term independent of $\\mu, \\sigma$, is\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{p(Z,\\pi)} \\log p(X,Z,\\pi; \\mu, \\sigma) &= \\mathbb{E}_{p(Z,\\pi)} \\log\\bigg( Dir(\\pi|\\alpha) \\prod_{i=1}^N \\prod_{k=1}^K \\big[\\pi_k \\mathcal{N}(x_i|\\mu_k, \\sigma_k)\\big]^{z_{ik}}\\bigg) \\\\\n",
    "&= \\mathbb{E}_{p(Z,\\pi)} \\bigg[ \\log Dir(\\pi|\\alpha) + \\sum_{i=1}^N\\sum_{k=1}^K z_{ik}\\log\\pi_k + z_{ik}\\log \\mathcal{N}(x_i|\\mu_k,\\sigma_k) \\bigg]\\\\\n",
    "&= \\sum_{i=1}^N\\sum_{k=1}^K \\mathbb{E}_{p(Z)}z_{ik}\\log \\mathcal{N}(x_i|\\mu_k,\\sigma_k) + \\text{const}\\\\\n",
    "&=\\sum_{i=1}^N\\sum_{k=1}^K \\mathbb{E}_{p(Z)}z_{ik} \\bigg( -\\log \\sigma_k - \\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\\bigg) + \\text{const}\\\\\n",
    "&=\\sum_{i=1}^N\\sum_{k=1}^K\\gamma_{ik} \\bigg( -\\log \\sigma_k - \\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}\\bigg) + \\text{const} \\\\\n",
    "&= \\sum_{i=1}^N\\sum_{k=1}^K\\gamma_{ik} \\bigg( \\frac{1}{2}\\log \\lambda_k - \\frac{(x_i-\\mu_k)^2}{2}\\lambda_k\\bigg) + \\text{const}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of Dirichlet distribution does not depend on $\\mu$ and $\\sigma$, thus is regarded as constant. Variate with respect to $\\mu_k$, \n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N\\gamma_{ik} \\frac{x_i-\\mu_k}{\\sigma_k^2} = 0 \\implies \\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik}x_i}{\\sum_{i=1}^N\\gamma_{ik}}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variate with respect to $\\lambda_k = 1/\\sigma_k^2$,\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N\\gamma_{ik}\\bigg(\\frac{1}{\\lambda_k} - (x_i-\\mu_k)^2\\bigg) = 0 \\implies \\lambda_k = \\frac{\\sum_{i=1}^N\\gamma_{ik}}{\\sum_{i=1}^N\\gamma_{ik}(x_i-\\mu_k)^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of \"GMM with Dirichlet Prior\"\n",
    "\n",
    "- inialize $q(Z)$ and $\\alpha$ \n",
    "- initialize ELBO (optional)\n",
    "- while not converged: \n",
    "    - E-step : update $\\alpha$, $q(Z)$ \n",
    "    - M-step : update $\\mu$, $\\lambda$ \n",
    "    - break if change in ELBO < tolerance\n",
    "\n",
    "Here, I will write a short code to implement the algorithm to test on 1D data generated above. In the next note, I will write a more detailed version to cover multi-dimensional case and add back the basic GMM, very much like the sklearn GMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import digamma\n",
    "\n",
    "class Dirichlet_GMM1D:\n",
    "    def __init__(self, n_components=1, max_iter=100):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X):\n",
    "        self._initialize(X)\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            self._e_step(X)\n",
    "            self._m_step(X)\n",
    "        \n",
    "    def _initialize(self, X):\n",
    "        n_samples, _ = X.shape\n",
    "        self.alpha = np.full(shape=self.n_components, fill_value=1./self.n_components)\n",
    "        self.qz = np.zeros(shape=(n_samples, self.n_components))\n",
    "        for n in range(n_samples):\n",
    "            k = np.random.randint(0, self.n_components)\n",
    "            self.qz[n][k] = 1\n",
    "\n",
    "        indices = np.random.randint(0, n_samples, size=self.n_components)\n",
    "        self.means_ = [X[i] for i in indices]\n",
    "        self.precisions_ = [1./np.var(X) for _ in range(self.n_components)]\n",
    "        \n",
    "\n",
    "    def _e_step(self, X):\n",
    "\n",
    "        nk = np.sum(self.qz, axis=0)\n",
    "        for k in range(self.n_components):\n",
    "            self.alpha[k] += nk[k]\n",
    "        \n",
    "        likelihood = np.zeros(shape=self.qz.shape)\n",
    "        for k in range(self.n_components):\n",
    "            distribution = multivariate_normal(mean=self.means_[k], cov=1./self.precisions_[k])\n",
    "            likelihood[:, k] = digamma(self.alpha[k]) - digamma(np.sum(self.alpha)) + np.log(distribution.pdf(X))\n",
    "\n",
    "        numerator = np.exp(likelihood)\n",
    "        denominator = np.sum(numerator, axis=1)[:, np.newaxis]\n",
    "        self.qz = numerator / denominator\n",
    "\n",
    "        \n",
    "    def _m_step(self, X):\n",
    "        nk = np.sum(self.qz, axis=0)\n",
    "\n",
    "        self.means_ = np.dot(self.qz.T, X) / nk[:, np.newaxis]\n",
    "        var = np.empty(shape=self.n_components)\n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means_[k]\n",
    "            var[k] = np.dot(self.qz[:, k] * diff.T, diff) / nk[k]\n",
    "        self.precisions_ = 1. / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.45178719]\n",
      " [1.69957529]\n",
      " [2.08488072]]\n",
      "[[1.58576205]\n",
      " [3.5822724 ]\n",
      " [2.23316051]]\n"
     ]
    }
   ],
   "source": [
    "gmm = Dirichlet_GMM1D(3, 1000)\n",
    "gmm.fit(X)\n",
    "\n",
    "print(gmm.means_)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d3cd1669c9bd0bc4a10448fec43c91475e59e3af6819e48dac91adb18c81ad3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
