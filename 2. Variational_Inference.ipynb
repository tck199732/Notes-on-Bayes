{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "When there is no conjugacy between prior and posterior, we need to make approximation to the posterior as it can't be derived in closed form. Variational Inference refers to the approximation $q(\\theta) \\approx p(\\theta|x)$ such that the Kullback-Leibler (KL) divergence is minimized, i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "q^*(\\theta) = \\argmin_{q(\\theta)} KL(q(\\theta)||p(\\theta|x)) \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the KL divergence is defined as \n",
    "\\begin{equation*}\n",
    "KL(q(\\theta)||p(\\theta|x)) = \\int q(\\theta) \\log \\frac{q(\\theta)}{p(\\theta|x)} d\\theta\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first glance to the equation is that if $q(\\theta) = p(\\theta|x)$ then the KL divergence equals zero. Under the same domain, it is a good measure of the difference between $q$ and $p$. Note also that \n",
    "\n",
    "1. $KL(q||p) \\neq KL(p||q)$\n",
    "2. $KL(q||p) \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergence depends on the posterior which is what we want to calculate in the first place. Luckily we have the following math trick:\n",
    "\\begin{align*}\n",
    "KL(q(\\theta)||p(\\theta|x)) &= \\int q(\\theta) \\log \\frac{q(\\theta)}{p(\\theta|x)} d\\theta\\\\\n",
    "&= \\int q(\\theta)\\log q(\\theta)d\\theta - \\int q(\\theta) \\log p(\\theta|x) d\\theta\\\\\n",
    "&= \\mathbb{E}_{q(\\theta)} \\log q(\\theta) - \\mathbb{E}_{q(\\theta)} \\log \\frac{p(\\theta,x)}{p(x)}\\\\\n",
    "&= \\mathbb{E}_{q(\\theta)} \\log q(\\theta) - \\mathbb{E}_{q(\\theta)} \\log p(\\theta,x) + \\mathbb{E}_{q(\\theta)}\\log p(x)\\\\\n",
    "&=\\mathbb{E}_{q(\\theta)} \\log q(\\theta) - \\mathbb{E}_{q(\\theta)} \\log p(\\theta,x) + \\log p(x)\\\\\n",
    "&= -L(q(\\theta)) + \\log p(x) \n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the RHS, we can compute the joint probability $p(\\theta,x)$ and we do not care about $p(x)$ as it does not depend on $q(\\theta)$. Therefore, minimizing $KL(q||p)$ is equivalent to maximizing $L(q)$. The optimization problem becomes:\n",
    "\\begin{equation*}\n",
    "    q^*(\\theta) = \\argmax_{q(\\theta)} \\mathbb{E}_{q(\\theta)} \\log \\frac{p(\\theta, x)}{q(\\theta)}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $L(q)$ is called Evidence Lower Bound (ELBO) as $KL(q||p) \\ge 0$ and thus $\\log p(x)\\ge L(q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean field approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, $\\theta$ refers to all varaibles, including latent variables (e.g. gaussian mixture model). It would be difficult to optimize with repect to all variables at the same time. Similar to physics problem, mean field approximation allows us to neglect correlation between variables. Let m be the number of variables, then \n",
    "\\begin{equation*}\n",
    "q(\\theta) = \\prod_{j=1}^m q_j(\\theta_j)\n",
    "\\end{equation*}\n",
    "Obviously the variables are independent of each other now. The ELBO becomes\n",
    "\\begin{equation*}\n",
    "L(q(\\theta)) = \\mathbb{E}_{q(\\theta)} \\log p(\\theta,x) - \\mathbb{E}_{q(\\theta)} \\sum_{j=1}^m\\log q_j(\\theta_j)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block coordinate assent\n",
    "Same idea as greedy algorithm. At each step, fix all other variables $\\theta_j\\neq\\theta_k$ and optimize with respect to $\\theta_k$. For $\\theta_k$, the ELBO reduces to \n",
    "\\begin{align*}\n",
    "L(q(\\theta)) & =\\mathbb{E}_{q(\\theta)} \\log p(\\theta,x) - \\mathbb{E}_{q_k(\\theta_k)} \\log q_k(\\theta_k) + \\text{const}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule $p(\\theta,x) = p(\\theta_k|\\theta_{-k},x) p(\\theta_{-k}|x) p(x)$, we have\n",
    "\\begin{align*}\n",
    "L(q(\\theta)) & =\\mathbb{E}_{q(\\theta)} \\bigg[\\log p(\\theta_k|\\theta_{-k},x)  + \\log p(\\theta_{-k}|x) + \\log p(x)\\bigg] - \\mathbb{E}_{q_k(\\theta_k)} \\log q_k(\\theta_k) + \\text{const}\\\\\n",
    "&= \\mathbb{E}_{q(\\theta)}\\log p(\\theta_k|\\theta_{-k},x) + \\mathbb{E}_{q(\\theta)}\\log p(\\theta_{-k}|x) + \\log p(x)- \\mathbb{E}_{q_k(\\theta_k)} \\log q_k(\\theta_k) + \\text{const}\\\\\n",
    "&= \\mathbb{E}_{q_k(\\theta_k)}\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta_k|\\theta_{-k},x)- \\mathbb{E}_{q_k(\\theta_k)} \\log q_k(\\theta_k) + \\text{const}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each optimization for $\\theta_k$ is subject to the constraint\n",
    "\\begin{equation*}\n",
    "\\int q_k(\\theta_k) d\\theta_k = 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the Lagrange multiplier, we have \n",
    "\\begin{equation*}\n",
    "    F(q_k) = \\int q_k(\\theta_k)\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta_k|\\theta_{-k},x) d\\theta_k - \\int q_k(\\theta_k) \\log q_k(\\theta_k) d\\theta_k - \\lambda \\bigg(\\int q_k(\\theta_k)d \\theta_k -1 \\bigg)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varaiating with respect to $q_k$ and $\\lambda$,\n",
    "\\begin{gather*}\n",
    "\\frac{\\delta F}{\\delta q_k} = \\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta_k|\\theta_{-k},x) - \\log q_k(\\theta_k) + 1- \\lambda = 0 \\\\ \n",
    "\\frac{\\delta F}{\\delta \\lambda} = \\int q_k(\\theta_k)d\\theta_k - 1 = 0\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first equation gives:\n",
    "\\begin{equation*}\n",
    "q_k(\\theta_k) = \\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta_k|\\theta_{-k},x)\\bigg] \\exp(1-\\lambda)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second equation just states the normalization, i.e.\n",
    "\\begin{equation*}\n",
    "q_k(\\theta_k) = \\frac{\\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta_k|\\theta_{-k},x)\\bigg]}{\\int \\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta_k|\\theta_{-k},x)\\bigg]d\\theta_k}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the conditional probability can be written as $p(\\theta_k|\\theta_{-k},x) = p(\\theta, x) / p(\\theta_{-k},x)$, the above expression can be rewritten as \n",
    "\\begin{equation*}\n",
    "q_k(\\theta_k) = \\frac{\\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta,x) \\bigg]}{\\int \\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta,x)  \\bigg]d\\theta_k}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm : Coordinate Ascent Pseudo-Code for Variational Inference\n",
    "\n",
    "Input : data, model $p(x,\\theta)$ \n",
    "\n",
    "Output : $q(\\theta)$\n",
    "\n",
    "1. Initialize $q(\\theta) = \\prod_{k=1}^m q_k(\\theta_k)$\n",
    "2. while not convergence, update $q_k(\\theta_k) \\propto \\exp\\bigg[\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta,x) \\bigg] $ for each k. \n",
    "3. calculate ELBO\n",
    "4. repeat until convergence.\n",
    "\n",
    "Note that in step 2 the update depends on the fact that we could calculate $\\mathbb{E}_{q_{-k}(\\theta_{-k})}\\log p(\\theta,x)$ for those fixed varaiables. That is to say, this algorithm is valid only if the prior and posterior satisfy conditional conjugacy, i.e. there is conjugacy for each \\theta_k while fixing other \\theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
